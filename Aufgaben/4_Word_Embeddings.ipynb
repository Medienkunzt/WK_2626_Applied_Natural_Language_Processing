{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word embedding model based on the Wikipedia corpus. Use the gensim library for this. Use the CBOW algorithm.\n",
    "\n",
    "# load wikipedia corpus\n",
    "with open(\"../Data/Wikipedia1M/Wikipedia1M.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# clean text\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9äöüÄÖÜß.,!?]', ' ', text) # remove all special characters\n",
    "    text = re.sub(r' +', ' ', text) # remove multiple spaces    \n",
    "    text = text.strip() # remove leading and trailing spaces\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0,7', 'prozent', 'stammen', 'von', 'zwei', 'oder', 'mehr', 'ethnien', 'ab', '.'], ['0', 'bedeutet', ',', 'dass', 'der', 'strahlengang', 'frei', 'ist', ',', 'der', 'füllstand', 'also', 'unter', 'der', 'grenze', 'liegt', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for sent in sent_tokenize(text, language='german'):\n",
    "    sentence = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for word in word_tokenize(sent):\n",
    "        sentence.append(word)\n",
    "\n",
    "    sentences.append(sentence)\n",
    "\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('personen', 0.7208806872367859), ('patienten', 0.7111926674842834), ('leute', 0.6857714056968689), ('tiere', 0.6834532022476196), ('juden', 0.6802852153778076), ('frauen', 0.6708083748817444), ('toten', 0.6684311032295227), ('männer', 0.6412639021873474), ('familien', 0.6326362490653992), ('tieren', 0.6319646239280701)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(sentences, min_count=1, vector_size=100, window=5)\n",
    "\n",
    "print(model1.wv.most_similar(\"menschen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function cossim(w1, w2) that calculates the cosine similarity between two vectors.\n",
    "import numpy as np\n",
    "\n",
    "def cossim(w1, w2):\n",
    "    return np.dot(w1, w2) / (np.linalg.norm(w1) * np.linalg.norm(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43498954,  0.46417958, -0.32569712,  0.24039018,  0.09296236,\n",
       "       -0.18372223, -0.35952717, -0.01104112,  0.21545486,  0.05401026,\n",
       "        0.09859841, -1.0155745 , -0.04165111,  0.41147262, -0.02257644,\n",
       "       -0.33504266,  0.68557876, -0.56821156, -0.18373854, -0.04167995,\n",
       "       -0.32734373,  0.14076912,  0.25999382, -0.07807504, -0.06900781,\n",
       "        0.26529306, -0.33303908,  0.13399129, -0.2790622 , -0.15900359,\n",
       "       -0.12758987,  0.19967936,  0.14941199, -0.4179893 , -0.52512467,\n",
       "       -0.15849856,  0.02963061, -0.25379038,  0.15074363, -0.28883052,\n",
       "       -0.14219615,  0.05503381, -0.69518065,  0.16264796,  0.14656578,\n",
       "        0.1322394 , -0.63289493,  0.43466383, -0.49597937,  0.37240455,\n",
       "       -0.12083096,  0.12206904, -0.03147397, -0.01720257,  0.39515272,\n",
       "        0.23616661,  0.05914237,  0.08096733,  0.01497447,  0.17408152,\n",
       "       -0.31952187, -0.26104748,  0.31906077, -0.14959455, -0.31884384,\n",
       "        0.48969546,  0.5502955 ,  0.26867366, -0.06523318,  0.49465284,\n",
       "       -0.04152954,  0.51119053,  0.2946636 , -0.51790345,  0.17306557,\n",
       "        0.01767418,  0.13291234, -0.0959901 , -0.25870502, -0.3108183 ,\n",
       "       -0.43938   , -0.39708975,  0.00644344,  0.39144656, -0.06103064,\n",
       "       -0.12553816,  0.05622042,  0.03618387,  0.07387726,  0.36387032,\n",
       "        0.3758785 , -0.25093022, -0.32140768, -0.24612685,  0.22346836,\n",
       "        0.23590267,  0.21359546,  0.34344387, -0.01973419,  0.11314419],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv[\"jupiter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6917642\n"
     ]
    }
   ],
   "source": [
    "# Using your cossim function, calculate the similarity between the words \"house\" and \"garden\".\n",
    "print(cossim(model1.wv[\"haus\"], model1.wv[\"garten\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chirurg', 0.8540403842926025), ('jurist', 0.8379354476928711), ('klavierlehrer', 0.8327844142913818), ('filmschauspieler', 0.8301833868026733), ('journalist', 0.8253464698791504), ('korrespondent', 0.8252529501914978), ('jugendlicher', 0.824674129486084), ('sportlehrer', 0.8196660876274109), ('fotograf', 0.813925564289093), ('theologe', 0.8106011748313904)] \n",
      "\n",
      "[('fotograf', 0.8436353802680969), ('jurist', 0.836018979549408), ('journalist', 0.8315922617912292), ('assistent', 0.82225501537323), ('chirurg', 0.8182920217514038), ('komponist', 0.8147286772727966), ('hochschullehrer', 0.8128437995910645), ('prediger', 0.8122571110725403), ('anwalt', 0.8104436993598938), ('ingenieur', 0.806882917881012)] \n",
      "\n",
      "[('komponist', 0.7939950227737427), ('drehbuchautor', 0.792672336101532), ('fotograf', 0.7888911366462708), ('chirurg', 0.7885728478431702), ('pianist', 0.7838389873504639), ('journalist', 0.7747824192047119), ('jurist', 0.7714508771896362), ('geschäftsmann', 0.7703101634979248), ('anwalt', 0.7668988108634949), ('dirigent', 0.764979362487793)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are the k = 10 most similar words to \"student\"?\n",
    "print(model1.wv.most_similar(\"student\", topn=10), \"\\n\")\n",
    "\n",
    "\n",
    "####################################\n",
    "########## Just for fun ############\n",
    "# What are the k = 10 most similar words to \"student\" and \"teacher\"?\n",
    "print(model1.wv.most_similar(positive=[\"student\", \"lehrer\"], topn=10), \"\\n\")\n",
    "\n",
    "# What are the k = 10 most similar words to \"student\" and \"teacher\" but not \"school\"?\n",
    "print(model1.wv.most_similar(positive=[\"student\", \"lehrer\"], negative=[\"schule\"], topn=10), \"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58e9a580e33e381d18751b4bd6d357b368bf1dd5dbcb6db220004eb9fac133a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
