{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a word embedding model based on the Wikipedia corpus. Use the gensim library for this. Use the CBOW algorithm.\n",
    "\n",
    "# load wikipedia corpus\n",
    "with open(\"./data/Wikipedia1M.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9äöüÄÖÜß.,!?]', ' ', text) # remove all special characters\n",
    "    text = re.sub(r' +', ' ', text) # remove multiple spaces\n",
    "    text = text.strip() # remove leading and trailing spaces\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0,7', 'prozent', 'stammen', 'von', 'zwei', 'oder', 'mehr', 'ethnien', 'ab', '.'], ['0', 'bedeutet', ',', 'dass', 'der', 'strahlengang', 'frei', 'ist', ',', 'der', 'füllstand', 'also', 'unter', 'der', 'grenze', 'liegt', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for sent in sent_tokenize(text, language='german'):\n",
    "    sentence = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for word in word_tokenize(sent):\n",
    "        sentence.append(word)\n",
    "\n",
    "    sentences.append(sentence)\n",
    "\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patienten', 0.7166435718536377), ('personen', 0.7116656303405762), ('juden', 0.6834033131599426), ('frauen', 0.6692613363265991), ('toten', 0.6628205180168152), ('tiere', 0.6598573327064514), ('leute', 0.6567414402961731), ('zuschauer', 0.6427351236343384), ('tieren', 0.6283061504364014), ('familien', 0.6277753114700317)]\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(sentences, min_count=1, vector_size=100, window=5)\n",
    "\n",
    "print(model1.wv.most_similar(\"menschen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function cossim(w1, w2) that calculates the cosine similarity between two vectors.\n",
    "def cossim(w1, w2):\n",
    "    return np.dot(w1, w2) / (np.linalg.norm(w1) * np.linalg.norm(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66897345  0.3146293  -0.24482161 -0.30263424 -0.06712674 -0.4251206\n",
      " -0.06840678  0.15362541 -0.06133319  0.14602293 -0.03685103 -0.9016724\n",
      " -0.20244004  0.17904517 -0.06902999 -0.38307574  0.10035982 -0.37113482\n",
      "  0.08088989  0.03854061 -0.09502782 -0.01011228  0.36522645  0.2535236\n",
      " -0.19787331 -0.32235172 -0.41177094  0.29512537  0.18478148  0.28647685\n",
      "  0.3257135  -0.05667533 -0.08776842 -0.47782192 -0.39685807 -0.49679235\n",
      "  0.15818967 -0.23659107  0.13748074 -0.39720163 -0.12495781 -0.0788565\n",
      " -0.28649434  0.03402524 -0.01293174  0.27510366 -0.48114207 -0.12199854\n",
      " -0.11865503  0.27582386  0.41232038 -0.21879673  0.09524512  0.0180094\n",
      " -0.01084151  0.15243421  0.21526974  0.16709802  0.05353617  0.30078155\n",
      " -0.02181968 -0.26701984  0.50182086  0.17582746 -0.5328402   0.546213\n",
      "  0.30934012  0.22113268  0.09473934  0.5174007  -0.23232757  0.29847744\n",
      "  0.3659659  -0.49686906 -0.07686497 -0.10666391 -0.1528373  -0.326977\n",
      " -0.04860533 -0.65139276 -0.30093902 -0.32122836 -0.17592864  0.4116079\n",
      "  0.4085945   0.02583891 -0.03204695 -0.08931249  0.64092594 -0.06314161\n",
      "  0.42159814 -0.16332778 -0.02845871 -0.14127247  0.5290666   0.08976039\n",
      "  0.18227997  0.2995391   0.00554865  0.15766479]\n"
     ]
    }
   ],
   "source": [
    "# show vector for word \"jupiter\"\n",
    "print(model1.wv[\"jupiter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68594307\n"
     ]
    }
   ],
   "source": [
    "# Using your cossim function, calculate the similarity between the words \"house\" and \"garden\".\n",
    "print(cossim(model1.wv[\"haus\"], model1.wv[\"garten\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fotograf', 0.8483164310455322), ('chirurg', 0.828510046005249), ('jurist', 0.8269718289375305), ('journalist', 0.8268405199050903), ('komponist', 0.8239739537239075), ('dirigent', 0.813700795173645), ('filmschauspieler', 0.8116052746772766), ('klavierlehrer', 0.8093410730361938), ('korrespondent', 0.8085117340087891), ('jugendlicher', 0.8058826327323914)] \n",
      "\n",
      "[('fotograf', 0.8536942005157471), ('assistent', 0.8467719554901123), ('hochschullehrer', 0.840185821056366), ('journalist', 0.8357746601104736), ('jurist', 0.8344106078147888), ('landwirt', 0.8292034864425659), ('komponist', 0.8276830315589905), ('anwalt', 0.8225088119506836), ('bibliothekar', 0.8152868151664734), ('zeichner', 0.8130288124084473)] \n",
      "\n",
      "[('drehbuchautor', 0.8352169394493103), ('komponist', 0.8148390650749207), ('fotograf', 0.8134076595306396), ('dirigent', 0.8126635551452637), ('journalist', 0.8066327571868896), ('jurist', 0.8039557337760925), ('übersetzer', 0.7850349545478821), ('pianist', 0.7848497033119202), ('regisseur', 0.7781088948249817), ('assistent', 0.7750979065895081)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What are the k = 10 most similar words to \"student\"?\n",
    "print(model1.wv.most_similar(\"student\", topn=10), \"\\n\")\n",
    "\n",
    "\n",
    "####################################\n",
    "########## Some more examples ######\n",
    "# What are the k = 10 most similar words to \"student\" and \"teacher\"?\n",
    "print(model1.wv.most_similar(positive=[\"student\", \"lehrer\"], topn=10), \"\\n\")\n",
    "\n",
    "# What are the k = 10 most similar words to \"student\" and \"teacher\" but not \"school\"?\n",
    "print(model1.wv.most_similar(positive=[\"student\", \"lehrer\"], negative=[\"schule\"], topn=10), \"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58e9a580e33e381d18751b4bd6d357b368bf1dd5dbcb6db220004eb9fac133a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
