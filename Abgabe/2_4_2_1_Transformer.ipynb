{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', '##ER', '##T', 'is', 'at', 'its', 'core', 'a', 'transform', '##er', 'language', 'model', 'with', 'a', 'variable', 'number', 'of', 'en', '##code', '##r', 'layers', 'and', 'self', '-', 'attention', 'heads', '.', 'The', 'architecture', 'is', 'almost', 'identical', 'to', 'the', 'original', 'transform', '##er', 'implementation', 'in', 'V', '##as', '##wan', '##i', 'et', 'al', '.', 'B', '##ER', '##T', 'was', 'pre', '##tra', '##ined', 'on', 'two', 'tasks', ':', 'language', 'modeling', '(', '15', '%', 'of', 'token', '##s', 'were', 'masked', 'and', 'B', '##ER', '##T', 'was', 'trained', 'to', 'predict', 'them', 'from', 'context', ')', 'and', 'next', 'sentence', 'prediction', '(', 'B', '##ER', '##T', 'was', 'trained', 'to', 'predict', 'if', 'a', 'chosen', 'next', 'sentence', 'was', 'probable', 'or', 'not', 'given', 'the', 'first', 'sentence', ')', '.', 'As', 'a', 'result', 'of', 'the', 'training', 'process', ',', 'B', '##ER', '##T', 'learns', 'context', '##ual', 'em', '##bed', '##ding', '##s', 'for', 'words', '.', 'After', 'pre', '##tra', '##ining', ',', 'which', 'is', 'computational', '##ly', 'expensive', ',', 'B', '##ER', '##T', 'can', 'be', 'fine', '##tun', '##ed', 'with', 'fewer', 'resources', 'on', 'smaller', 'data', '##sets', 'to', 'op', '##ti', '##mize', 'its', 'performance', 'on', 'specific', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"BERT is at its core a transformer language model with a variable number of encoder layers and self-attention heads. The architecture is almost identical to the original transformer implementation in Vaswani et al. BERT was pretrained on two tasks: language modeling (15% of tokens were masked and BERT was trained to predict them from context) and next sentence prediction (BERT was trained to predict if a chosen next sentence was probable or not given the first sentence). As a result of the training process, BERT learns contextual embeddings for words. After pretraining, which is computationally expensive, BERT can be finetuned with fewer resources on smaller datasets to optimize its performance on specific tasks.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
